Q1. The predicutive model which i am building i am using linear regresstion, lasso, ridge, elasticnet. Let's know What are these..!!
->  1Ô∏è‚É£ Linear Regression
This is the most basic and simple regression technique. The model draws a straight line that best fits the data, helping us make predictions.

üìå Example:
If we look at the relationship between temperature and ice cream sales, as the temperature increases, ice cream sales also increase. 
Linear Regression will draw a straight line to predict this trend.

2Ô∏è‚É£ Lasso Regression (L1 Regularization)
Lasso is an upgraded version of Linear Regression. It removes unimportant variables (features) by setting them to zero, making the model simpler and cleaner.

üìå Example:
If you're building a house price prediction model and your dataset has 100 features (like area, rooms, location, parking, etc.), 
Lasso will keep only the important features and set the rest to zero.

3Ô∏è‚É£ Ridge Regression (L2 Regularization)
Ridge is also an improved version of Linear Regression, but it does not remove features. Instead, it makes their impact smaller by reducing their coefficients.
This helps solve the overfitting problem.

üìå Example:
If you're predicting student marks based on attendance, study hours, and extra activities, but your model is overfitting (performing well on training data but failing on new data), 
Ridge will adjust the coefficients to make the model generalize better.

4Ô∏è‚É£ ElasticNet Regression
ElasticNet is a combination of Lasso and Ridge. This means it can remove unimportant features (like Lasso) and also control the impact of remaining features (like Ridge).

üìå Example:
If you're building a medical model to predict diseases in patients, and your dataset has many features, ElasticNet will automatically use the best approach to ensure accurate
predictions.

üí° TL;DR (Quick Summary)
‚úÖ Linear Regression ‚Äì Just fits a straight line to the data.
‚úÖ Lasso Regression ‚Äì Removes unnecessary features (sets them to zero).
‚úÖ Ridge Regression ‚Äì Doesn‚Äôt remove features, but reduces their impact.
‚úÖ ElasticNet Regression ‚Äì A mix of Lasso & Ridge (best of both).

------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Q2. What is Overfitting?
-> Overfitting happens when a machine learning model learns the training data too well, so much that it fails to perform well on new (unseen) data.

ü§Ø Think of it like this:
Imagine a student who memorizes all the answers for an exam without actually understanding the concepts. If the exact same questions appear in the exam, 
he will do great. But if the questions are slightly different, he will struggle!

How does this happen in ML?
The model becomes too complex and memorizes patterns from training data instead of learning general trends.
It performs very well on training data but poorly on test data.
Instead of learning, it remembers the data like a parrot ü¶ú.
How to detect overfitting?
‚úÖ High accuracy on training data but low accuracy on test data
‚úÖ Training error is very low, but test error is high
‚úÖ Model is too complex without real improvement in performance

How to avoid overfitting?
1Ô∏è‚É£ Use more data (More examples help the model learn better patterns)
2Ô∏è‚É£ Feature selection (Use only important inputs, remove unnecessary ones)
3Ô∏è‚É£ Regularization (Use Lasso, Ridge, or ElasticNet to prevent the model from becoming too complex)
4Ô∏è‚É£ Cross-validation (Train and test on multiple subsets of data to check if the model is generalizing well)

